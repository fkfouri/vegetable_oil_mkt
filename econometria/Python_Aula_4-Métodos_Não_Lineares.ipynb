{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn, rand, standard_t, normal, uniform\n",
    "from numpy import sqrt, arange, mean, std\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import leastsq                       # Minimize the sum of squares of a set of equations. leastsq internamente usa um método gradiente de Levenburg-Marquardt para minimizar a função objetivo\n",
    "from scipy.optimize import least_squares                 # least_squares finds a local minimum of the cost function F(x)\n",
    "from scipy.optimize import curve_fit                     # Use non-linear least squares to fit a function, f, to data\n",
    "import pandas as pd                                      # Carrega o pacote pandas e o chama de pd\n",
    "import numpy as np                                       # Carrega o pacote pandas e o chama de pd\n",
    "np.set_printoptions(precision=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLLS\n",
    "\n",
    "Suponha que queiramos estimar o seguinte modelo não linear:\n",
    "$$ y_i = \\beta_0 + x_i^{\\beta_1} + e_i$$\n",
    "\n",
    "Para isso, vamos usar $n=1000$ observações construídas artificialmente com $\\beta_0=0.5$ e $\\beta_1=1.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n  = 1000;\n",
    "e  = uniform(-1,1,n);                  # Simula n choques uniformes em [-1,1]\n",
    "x  = normal(10,2,n);                   # simula uma variável normal com média 10, std=2\n",
    "y  = .5  +  x**(1.5) + e               # constrói y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo a função objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLS_obj(beta, y, x):          # Cria um objeto \"NLLS_obj\" que contém a função objetivo do problema\n",
    "    b_0 = beta[0]\n",
    "    b_1 = beta[1]\n",
    "    return y - b_0 -  (x**b_1)     # retorna o resultado da conta y - b0 - b1 * (x**b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, precisamos definir um chute inicial. Ele precisa ser tal que y exista!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0   = np.array([1.0,1.0])        # Chute inicial\n",
    "Beta_nl = least_squares(NLLS_obj, beta0, args = (y, x))  # least_squares(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "Beta_nl = Beta_nl.x\n",
    "\n",
    "Beta_nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, precisamos calcular a jacobinada para estimarmos a variância de $\\beta$\n",
    "Note, inicialmente, que se trata de um modelo não linear, exatamente identificado e homocedástico. Além disso, \n",
    "\n",
    "$$\\theta=(\\beta_0,\\beta_1)^\\top$$\n",
    "$$ X_i = (1,x_i)^\\top$$\n",
    "\\begin{equation*}\n",
    "\tg(v_i,\\theta)=y_i-\\beta_0 -x_i^{\\beta_1}\n",
    "\\end{equation*}\n",
    "\n",
    "$$\\underset{(1\\times K)}{\\partial_\\theta g(v_i;\\theta)}=\\begin{bmatrix}\n",
    "-1 & - x_i^{\\beta_1}\\text{log}(x_i)\\\\ \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_11  = - np.ones((n,1))                     # Gradiente del_g1/del beta_0\n",
    "g_12  = - (x**Beta_nl[1])*(np.log(x));       # Gradient  del_g1/del beta_1\n",
    "g_12  = g_12[...,None]\n",
    "dG    = np.concatenate((g_11,g_12),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hat = (y - Beta_nl[0] - x**Beta_nl[1])\n",
    "sig_2 = e_hat.T@e_hat/(n-2)\n",
    "sig_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var_B_nl = sig_2 * inv(dG.T@dG)\n",
    "print(Var_B_nl)\n",
    "SE_nl    = sqrt(np.diag(Var_B_nl))\n",
    "SE_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLS_obj2(x,b_0,b_1):        # Cria um objeto \"NLLS_obj2\" que contém a função objetivo do problema\n",
    "    return b_0 + (x**b_1)        # retorna o resultado da conta b0 + b1 * (x**b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organizando as variáveis para o print\n",
    "np.set_printoptions(precision=4)\n",
    "popt, pcov = curve_fit(NLLS_obj2, x, y)\n",
    "\n",
    "print(\"Parameter estimation results:\")\n",
    "print(\"b_0 = \",\" %8.4f\" % popt[0],\" | b_1 = \",\" %8.4f\" % popt[1])\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Standard Deviations:\")\n",
    "print(sqrt(np.diag(pcov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Não linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{eqnarray*}\n",
    "\tg_1(v_i, \\theta_0)&=& z_{i1}(y_i-f(x_i,\\theta_0))= 0\\\\\n",
    "\tg_2(v_i, \\theta_0)&=& z_{i2}(y_i-f(x_i,\\theta_0))= 0\\\\\n",
    "\t\\vdots&=& \\vdots \\\\\n",
    "\tg_m(v_i, \\theta_0)&=& z_{im}(y_i-f(x_i,\\theta_0))= 0,\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "Vamos simplificar o modelo para fins de ilustração. -- ou precisaríamos modelar os instrumentos, o que foge ao escopo da aula.\n",
    "\n",
    "Defina o distúrbio aleatório por \n",
    "\n",
    "$$\\epsilon_i=y_i-\\beta_0 -x_i^{\\beta_0}$$\n",
    "\n",
    "Note que agora $b_1=b_0$ e, portando, temos um parâmetro e duas variáveis explicativas: a constante e $x$.\n",
    "\n",
    "Com isso, podemos utilizar o GMM para recuperar $\\beta_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja \n",
    "$$\\theta=\\beta_0$$\n",
    "com \n",
    "$$ X_i = x_i$$\n",
    "e o conjunto de instrumentos\n",
    "$$ Z_i = (1,x_i)^\\top$$\n",
    "\n",
    "Note-se, inicialmente, que se trata de um modelo não linear, sobre-identificado e homocedástico. Para ter uma solução, assume-se que $x_i>0$.  As condições de momento do problema são as seguintes:\n",
    "\n",
    "\\begin{equation*}\n",
    "\tg(v_i,\\theta_0)=Z_i(y_i-\\beta_0 -x_i^{\\beta_0})=\\begin{pmatrix}\n",
    "\t\t1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "\t\tx_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "\t\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Com isso, o estimador de GMM de $\\theta_0=\\beta_0$ é dado pela minimização do seguinte problema:\n",
    "\n",
    "$$ \\underset{(K\\times1)}{ \\theta_{gmm}} = \\underset{\\theta_0}{\\text{argmin }} \\underset{(1\\times m)}{\\Bigg(\\sum_{i=1}^n g(v_i;\\theta_0)\\Bigg)^\\prime} \\underset{(m\\times m)}{W_n} \\underset{(m\\times 1)}{\\Bigg(\\sum_{i=1}^n g(v_i;\\theta_0)\\Bigg)} $$\n",
    "\n",
    "ou seja, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\t\\hat{\\theta}=\\underset{\\theta_0}{\\text{arg min}} \\begin{pmatrix}\\frac{1}{n}\\sum_{i=1}^n\\begin{pmatrix}\n",
    "\t\ty_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "\t\tx_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "\t\\end{pmatrix}\\end{pmatrix}^\\top W_n \t\\begin{pmatrix}\\frac{1}{n}\\sum_{i=1}^n\\begin{pmatrix}\n",
    "\ty_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "\tx_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "\\end{pmatrix} \\end{pmatrix},\n",
    "\\end{equation*}\n",
    "em que $W_n$ é uma matriz quadrada e positiva semi-definida.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se mostrar que, assimptoticamente, vale que:\n",
    "\n",
    "\\begin{eqnarray*} \\small\n",
    "\t\t&&\\sqrt{n}(\\hat{\\theta}-\\theta _{0})\\overset{d}{\\rightarrow }N(0,\\left[\n",
    "\t\tE(\\partial _{\\theta }g(v_{i};\\theta _{0}))^{\\top }WE(\\partial _{\\theta\n",
    "\t\t}g(v_{i};\\theta _{0}))\\right]^{-1}E(\\partial _{\\theta }g(v_{i};\\theta\n",
    "\t\t_{0}))^{\\top }WE(g(v_{i};\\theta _{0})g(v_{i};\\theta _{0}))^{\\top } \\\\\n",
    "\t\t&&{\\quad \\quad  \\quad\\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad }WE(\\partial _{\\theta\n",
    "\t\t}g(v_{i};\\theta _{0}))\\left[ E(\\partial _{\\theta }g(v_{i};\\theta\n",
    "\t\t_{0}))^{\\top }WE(\\partial _{\\theta }g(v_{i};\\theta _{0}))\\right] ^{-1})\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabe-se que o GMM torna-se eficiente quando a matriz de ponderação $W$ é dada pela \n",
    "inversa da variância dos momentos,\n",
    "\\begin{eqnarray*}\n",
    "\tW^{\\ast } &=&\\left[ E(g(v_{i};\\theta _{0})g(v_{i};\\theta _{0}))^{\\prime }%\n",
    "\t\\right] ^{-1}\n",
    "\\end{eqnarray*}\n",
    "Neste caso, \n",
    "\\begin{equation*}\n",
    "\t\\sqrt{n}(\\hat{\\theta}-\\theta _{0})\\overset{d}{%\n",
    "\t\t\t\\rightarrow }N\\left( 0,\\left[ E(\\partial _{\\theta }g(v_{i};\\theta\n",
    "\t\t_{0}))^{\\top }\\left[ E(g(v_{i};\\theta _{0})g(v_{i};\\theta _{0}))^{\\top }%\n",
    "\t\t\\right] ^{-1}E(\\partial _{\\theta }g(v_{i};\\theta _{0}))\\right] ^{-1}\\right) %\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, usando \n",
    "$$W_n= E \\Bigg[\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}^\\top\\Bigg]^{-1} $$ \n",
    "    \n",
    "e a Jacobiana\n",
    "    \n",
    "$$G_0=\\begin{pmatrix}\n",
    "-1-x_{i}^{\\beta_0}\\text{ln}(x_{i})\\\\\n",
    "-x_i-x_{i}{x_{i}^\\top}^{\\beta_0}\\text{ln}(x_{i})\n",
    "\\end{pmatrix}$$ \n",
    "a variância assintótica de $\\hat{\\theta}$ será:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\text{Avar}(\\hat{\\theta})=\\left[E\\begin{pmatrix}\n",
    "\t-1-x_{i}^{\\beta_0}\\text{ln}(x_{i})\\\\\n",
    "\t-x_{i}-x_{i}{x_{i}^top}^{\\beta_0}\\text{ln}(x_{i})\n",
    "\\end{pmatrix}^\\top E \\Bigg[\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}^\\top\\Bigg]^{-1} \n",
    "E\\begin{pmatrix}\n",
    "\t-1-x_{i}^{\\beta_0}\\text{ln}(x_{i})\\\\\n",
    "\t-x_{i}-x_{i}{x_{i}^\\top}^{\\beta_0}\\text{ln}(x_{i})\n",
    "\\end{pmatrix}\\right]^{-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, nosso problema é encontrar as contra partidas amostrais $W_{n}^{\\ast}$, $G_{0,n}$ tal que o estimador da variância assimptótica do estimador GMM eficiênte seja:\n",
    "\n",
    "\\begin{equation*}\n",
    "\tG_{0,n}^\\top E \\Bigg[\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}^\\top\\Bigg]^{-1}  G_{0,n}\n",
    "\\end{equation*}\n",
    "\n",
    "Ou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\t\\text{Avar}(\\hat{\\theta})=\\left[ \\frac{1}{n}\\sum_{i=1}^n\\begin{pmatrix}\n",
    "\t\t-1-x_{i}^{\\beta_0}\\text{ln}(x_{i})\\\\\n",
    "\t\t-x_{i}-x_{i}{x_{i}^\\top}^{\\beta_0}\\text{ln}(x_{i})\n",
    "\t\\end{pmatrix}^\\top \\frac{1}{n}\\sum_{i=1}^n \\Bigg[\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}\\begin{pmatrix}\n",
    "    1(y_i-\\beta_0 -x_i^{\\beta_0})\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}^\\top\\Bigg]^{-1} \n",
    "\t\\frac{1}{n}\\sum_{i=1}^n\\begin{pmatrix}\n",
    "\t\t-1-x_{i}^{\\beta_0}\\text{ln}(x_{i})\\\\\n",
    "\t\t-x_{i}-x_{i}{x_{i}^\\top}^{\\beta_0}\\text{ln}(x_{i})\n",
    "\t\\end{pmatrix}\\right]^{-1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retomando, vamos construir os dados com o novo conjunto de parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m  = 2                                  # Número de condições de momento\n",
    "n  = 1000;                              # Número de observações\n",
    "e  = uniform(-1,1,(n,1));               # Simula n choques uniformes em [-1,1]\n",
    "x  = normal(10,2,(n,1));                # simula uma variável normal com média 10, std=2\n",
    "y  = 1.5  +  x**(1.5) + e               # constrói y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo a função objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_obj(b0,y,x,W):                  # Cria um objeto \"GMM_obj\" que contém a função objetivo do problema\n",
    "    g_1 = mean(y - b0 - x**b0);\n",
    "    g_2 = mean(x*(y - b0 - x**b0));\n",
    "    g   = np.array([g_1,g_2]);\n",
    "    g   = np.reshape(g,(2,1));\n",
    "    J   = g.T @ W @ g\n",
    "    return  J[0]                         # retorna o valor da função objetivo J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W  = np.eye(m)                           # Chute inicial: W= I_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_Var(b0,y,x): \n",
    "    G_1  = mean(-1 - x**b0*np.log(x));   # Derivada da condição de momento 1 com relação à b_0\n",
    "    G_2  = mean(-x - x*x**b0*np.log(x)); # Derivada da condição de momento 2 com relação à b_0\n",
    "    G    = np.array([G_1,G_2]);          # Constrói G_0\n",
    "    G    = np.reshape(G,(2,1));          # Ajusta o objeto 1D para 2x1\n",
    "    ehat = y - b0 - x**b0;               # Computa os resíduos da regressão       \n",
    "    g_1  = (1*(ehat));                   # Condição de momento 1\n",
    "    g_2  = (x*(ehat));                   # Condição de momento 2\n",
    "    g    = np.concatenate((g_1,g_2),axis=1);       # Constrói g para n indivíduos. Portanto, mxn\n",
    "    igg  = 1/n*g.T@g\n",
    "    W    = inv(igg)\n",
    "    AVAR = inv(G.T @ W @ G)\n",
    "    return AVAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computando $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ini    = 0;\n",
    "Beta_GMM = least_squares(GMM_obj,b_ini, args = (y,x,W),verbose=1)  # least_squares(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "Beta_GMM.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAR_GMM = GMM_Var(Beta_GMM.x,y,x)\n",
    "SE_GMM   = sqrt(AVAR_GMM)\n",
    "SE_GMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos construir W e obter uma estimação mais precisa de $\\beta_0$ utilizando o 2SGMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os resíduos:\n",
    "beta_GMM = Beta_GMM.x\n",
    "ehat     = y - beta_GMM - x**beta_GMM; # resíduo da regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma 1: contra-partida das condições de momento\n",
    "z    = np.concatenate((np.ones((n,1)),x),axis=1)\n",
    "momt = z*ehat                     # Multiplicação elemento-por-elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma 2: Montando a matriz g item por item\n",
    "g_1  = (1*(ehat));                # Condição de momento 1\n",
    "g_2  = (x*(ehat));                # Condição de momento 2\n",
    "g    = np.concatenate((g_1,g_2),axis=1);       # Constrói g para n indivíduos. Portanto, mxn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(momt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igg  = 1/n*g.T@g\n",
    "W    = inv(igg)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ou W = inv(1/n*momt.T@momt)\n",
    "W = inv(1/n*(momt.T@momt))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ini      = 0;\n",
    "Beta_2SGMM = least_squares(GMM_obj,b_ini, args = (y,x,W),verbose=1)  # leastsq(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "Beta_2SGMM.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAR_2SGMM = GMM_Var(Beta_GMM.x,y,x)\n",
    "SE_2SGMM   = sqrt(AVAR_2SGMM)\n",
    "SE_2SGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM-Interativo\n",
    "\n",
    "Note que podemos continuar obtendo novas estimativas de $\\beta_0$, pois agora poderemos obter uma nota matriz $W=W(\\beta_{0,2SGMM})$.\n",
    "\n",
    "De fato, podemos obter uma sequência de novas estimativas até que $W$ convirja para um valor fixo.\n",
    "\n",
    "Há muitas formas para se implementar este estimador. Aqui, usaremos uma não tão elegante, mas que tem um carater pedagógico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta_GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "eps  = 1e-10                        # Define um valor suficientemente pequeno\n",
    "dist = 10                           # Define um valor positivo suficientemente grande\n",
    "Iter = 1;                           # Contador de iterações  \n",
    "Beta_iGMM  = copy.deepcopy(Beta_GMM)# Copia o objeto Beta_GMM\n",
    "Beta_iGMM.x=1;                      # Chute inicial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while dist > eps:\n",
    "    if (Iter  == 1):\n",
    "             W = np.eye(m)                    \n",
    "    W_ini     = W                    # na iteração i, define W_inicial como o W que estiver na memória\n",
    "    b_ini     = Beta_iGMM.x;          # na iteração i, define Beta_0_inicial como o Beta que estiver na memória\n",
    "    Beta_iGMM = least_squares(GMM_obj,b_ini, args = (y,x,W),verbose=1)  # leastsq(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "    b0        = Beta_iGMM.x          # Salva o vetor estimado na iteração i como b0   \n",
    "    ehat      = y - b0 - x**b0;      # Computa os resíduos da regressão       \n",
    "    g_1       = (1*(ehat));          # Condição de momento 1\n",
    "    g_2       = (x*(ehat));          # Condição de momento 2\n",
    "    g         = np.array([g_1,g_2]); # Constrói g\n",
    "    g         = np.reshape(g,(2,n)); # Ajusta o objeto 1D para 2x1 \n",
    "    igg       = 1/n*g@g.T\n",
    "    W         = inv(igg)\n",
    "    diff      = abs(np.reshape(W, -1)-np.reshape(W_ini, -1)) # Computa o valor absoluto da diferença entre W's\n",
    "    dist      = max(diff)\n",
    "    Iter      = Iter+1\n",
    "\n",
    "print(Iter)\n",
    "print(dist)\n",
    "print(b0-b_ini)\n",
    "print(b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 10                           # Define um valor positivo suficientemente grande\n",
    "Iter = 1;                           # Contador de iterações  \n",
    "Beta_iGMM  = copy.deepcopy(Beta_GMM)# Copia o objeto Beta_GMM\n",
    "Beta_iGMM.x=1;      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while dist > eps:\n",
    "    if (Iter  == 1):\n",
    "             W = np.eye(m)        \n",
    "    W_ini     = W                    # na iteração i, define W_inicial como o W que estiver na memória\n",
    "    b_ini     = Beta_iGMM.x;         # na iteração i, define Beta_0_inicial como o Beta que estiver na memória\n",
    "    Beta_iGMM = least_squares(GMM_obj,b_ini, args = (y,x,W),verbose=1)  # leastsq(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "    b0        = Beta_iGMM.x           # Salva o vetor estimado na iteração i como b0   \n",
    "    ehat      = y - b0 - x**b0;      # Computa os resíduos da regressão       \n",
    "    g_1       = (1*(ehat));          # Condição de momento 1\n",
    "    g_2       = (x*(ehat));          # Condição de momento 2\n",
    "    g         = np.array([g_1,g_2]); # Constrói g\n",
    "    g         = np.reshape(g,(2,n)); # Ajusta o objeto 1D para 2x1 \n",
    "    igg       = 1/n*g@g.T\n",
    "    W         = inv(igg)\n",
    "    diff      = abs(b0-b_ini) # Computa o valor absoluto da diferença entre Betas\n",
    "    dist      = max(diff)\n",
    "    Iter      = Iter+1\n",
    "\n",
    "print(Iter)\n",
    "print(dist)\n",
    "print(b0-b_ini)\n",
    "print(b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUGMM - Continuously updating GMM\n",
    "\n",
    "Aqui, a ideia é estimar $\\beta_0$ e $W(\\beta_0)$ ao mesmo tempo. Isso requer resolver o seguinte problema não-linear:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\theta=\\underset{\\theta_0}{\\text{arg min}}\\left\\{\\frac{1}{n}\\sum_{i=1}^n \\begin{pmatrix}\n",
    "\t\ty_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "\t\tx_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "\t\\end{pmatrix}^\\top \\frac{1}{n}\\sum_{i=1}^n \\Bigg[\\begin{pmatrix}\n",
    "    y_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}\\begin{pmatrix}\n",
    "    y_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "    x_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "    \\end{pmatrix}^\\top\\Bigg]^{-1}\\frac{1}{n}\\sum_{i=1}^n  \t\\begin{pmatrix}\n",
    "\ty_i-\\beta_0 -x_i^{\\beta_0}\\\\ \n",
    "\tx_i(y_i-\\beta_0 -x_i^{\\beta_0})\n",
    "\\end{pmatrix} \\right\\},\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Assim, definimos a nota função objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CUGMM_obj(b0,y,x):          # Cria um objeto \"GMM_obj\" que contém a função objetivo do problema\n",
    "    n    = len(x)\n",
    "    g_1  = (y - b0 - x**b0);\n",
    "    g_2  = (x*(y - b0 - x**b0));\n",
    "    g    = np.array([g_1,g_2]);\n",
    "    g    = np.reshape(g,(n,2));\n",
    "    momt = np.array([mean(g_1), mean(g_2)])\n",
    "    momt = np.reshape(momt,(2,1));\n",
    "    W    = inv(1/n*g.T@g)\n",
    "    J    = momt.T @ W @ momt\n",
    "    return  J[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ini      = Beta_iGMM.x;\n",
    "Beta_CUGMM = least_squares(CUGMM_obj,b_ini, args = (y,x),verbose=1)  # leastsq(fun, x0, param) Minimiza a função fun com chute inicial x0 e parâmetros param\n",
    "Beta_CUGMM.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAR_CUGMM = GMM_Var(Beta_CUGMM.x,y,x)\n",
    "SE_CUGMM   = sqrt(AVAR_CUGMM)\n",
    "SE_CUGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de sobreidentificação\n",
    "\n",
    "Seriam válidos os momentos que assumimos para o modelo?\n",
    "Se $m>K$, podemos testar se as condições de momento de fato possuem média 0.\n",
    "\n",
    "Ou seja, podemos testar se \n",
    "$$ H_0 : E[g(v_i;\\theta)] = 0$$\n",
    "contra\n",
    "$$ H_A : E[g(v_i;\\theta)] \\neq 0$$\n",
    "\n",
    "O teste é intuitivo: se $\\frac{1}{n}\\sum_{i=1}^n g(v_i\\theta)\\xrightarrow{p}0$, então uma forma quadrática desta expressão terá, necessariamente, um valor pequeno e positivo. Disso, segue que a estatística de tal teste é chi-quadrado.\n",
    "\n",
    "\n",
    "$$\\text{$J$-stat}= n\\bigg(\\frac{1}{n}\\sum_{i=1}^n g(v_i\\theta)\\bigg)' \\hat{W} \\bigg(\\frac{1}{n}\\sum_{i=1}^n g(v_i\\theta)\\bigg) \\sim\\chi^2_{m-K}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "z    = np.concatenate((np.ones((n,1)),x),axis=1)\n",
    "ehat = y - Beta_CUGMM.x - x**Beta_CUGMM.x; # resíduo da regressão\n",
    "g    = z*ehat\n",
    "W    = inv(1/n*g.T@g)\n",
    "momt = mean(g,axis=0)\n",
    "momt = np.reshape(momt,(m,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_stat  = n*((1/n*momt.T) @ W @ (1/n* momt))\n",
    "Jdf     = m-1;\n",
    "Jpvalue = 1-chi2.cdf(J_stat, Jdf)\n",
    "print(J_stat)\n",
    "print(Jpvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 313.656,
   "position": {
    "height": "333.646px",
    "left": "1354.29px",
    "right": "20px",
    "top": "117px",
    "width": "326px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
